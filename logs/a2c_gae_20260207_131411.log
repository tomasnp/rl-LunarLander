================================================================================
üöÄ A2C with GAE - Lunar Lander Training
================================================================================
Rollout steps:      2048
Max updates:        2500
GAE lambda:         0.95
Entropy annealing:  0.05 ‚Üí 0.001
Gradient clipping:  0.5
================================================================================


================================================================================
‚öôÔ∏è  CONFIGURATION
================================================================================
  env_id                    = LunarLander-v3
  seed                      = 42
  gamma                     = 0.99
  gae_lambda                = 0.95
  lr_policy                 = 0.0003
  lr_value                  = 0.0003
  entropy_coef_start        = 0.05
  entropy_coef_final        = 0.001
  value_coef                = 0.5
  rollout_steps             = 2048
  max_updates               = 2500
  eval_every                = 50
  eval_episodes             = 10
  hidden_size               = 128
  grad_clip                 = 0.5
  save_dir                  = checkpoints
  save_name                 = a2c_2048_2500.pt
  render_eval_human         = False
  record_video              = False
  video_dir                 = videos_record
  solved_mean_reward        = 200.0
  solved_window             = 100
================================================================================

[INFO] Device: cpu
[INFO] Training mode: A2C with GAE (rollout_steps=2048)
[INFO] PyTorch version: 2.8.0
[INFO] Gymnasium version: 1.2.3

Update   10 | return= -184.0 (n=217) | loss=16.042 | policy=-0.071 | value=32.224 | entropy=1.381 (coef=0.0498) | adv: Œº=-0.000 œÉ=1.000
Update   20 | return= -176.2 (n=407) | loss=15.149 | policy=-0.066 | value=30.430 | entropy=1.371 (coef=0.0496) | adv: Œº=0.000 œÉ=1.000
Update   30 | return= -141.1 (n=614) | loss=12.706 | policy=-0.078 | value=25.566 | entropy=1.367 (coef=0.0494) | adv: Œº=-0.000 œÉ=1.000
Update   40 | return= -121.5 (n=815) | loss=9.508 | policy=-0.069 | value=19.155 | entropy=1.349 (coef=0.0492) | adv: Œº=-0.000 œÉ=1.000
Update   50 | return= -130.3 (n=1000) | loss=10.012 | policy=-0.059 | value=20.142 | entropy=1.320 (coef=0.0490) | adv: Œº=0.000 œÉ=1.000
[EVAL] Update   50 | avg_return over 10 eps = -1386.6
[SAVE] New best model saved to: checkpoints/a2c_2048_2500.pt
Update   60 | return= -129.2 (n=1175) | loss=9.186 | policy=-0.055 | value=18.481 | entropy=1.300 (coef=0.0488) | adv: Œº=-0.000 œÉ=1.000
Update   70 | return= -129.4 (n=1334) | loss=8.552 | policy=-0.044 | value=17.192 | entropy=1.290 (coef=0.0486) | adv: Œº=-0.000 œÉ=1.000
Update   80 | return=  -96.6 (n=1506) | loss=7.549 | policy=-0.072 | value=15.242 | entropy=1.287 (coef=0.0484) | adv: Œº=-0.000 œÉ=1.000
Update   90 | return= -102.0 (n=1642) | loss=6.861 | policy=-0.074 | value=13.870 | entropy=1.289 (coef=0.0482) | adv: Œº=0.000 œÉ=1.000
Update  100 | return=  -95.5 (n=1776) | loss=8.548 | policy=-0.077 | value=17.251 | entropy=1.253 (coef=0.0480) | adv: Œº=-0.000 œÉ=1.000
[EVAL] Update  100 | avg_return over 10 eps = -2805.3
Update  110 | return=  -82.7 (n=1923) | loss=7.899 | policy=-0.083 | value=15.964 | entropy=1.242 (coef=0.0478) | adv: Œº=-0.000 œÉ=1.000
Update  120 | return=  -91.8 (n=2046) | loss=6.714 | policy=-0.072 | value=13.573 | entropy=1.234 (coef=0.0476) | adv: Œº=-0.000 œÉ=1.000
Update  130 | return=  -74.5 (n=2159) | loss=6.957 | policy=-0.079 | value=14.072 | entropy=1.225 (coef=0.0474) | adv: Œº=0.000 œÉ=1.000
Update  140 | return=  -59.2 (n=2294) | loss=7.630 | policy=-0.072 | value=15.404 | entropy=1.192 (coef=0.0472) | adv: Œº=0.000 œÉ=1.000
Update  150 | return=  -67.0 (n=2396) | loss=7.173 | policy=-0.064 | value=14.474 | entropy=1.187 (coef=0.0470) | adv: Œº=-0.000 œÉ=1.000
[EVAL] Update  150 | avg_return over 10 eps = -737.2
[SAVE] New best model saved to: checkpoints/a2c_2048_2500.pt
Update  160 | return=  -55.0 (n=2507) | loss=5.220 | policy=-0.093 | value=10.626 | entropy=1.243 (coef=0.0468) | adv: Œº=0.000 œÉ=1.000
Update  170 | return=  -57.7 (n=2612) | loss=7.300 | policy=-0.036 | value=14.673 | entropy=1.165 (coef=0.0466) | adv: Œº=0.000 œÉ=1.000
Update  180 | return=  -36.3 (n=2721) | loss=6.876 | policy=-0.045 | value=13.842 | entropy=1.164 (coef=0.0464) | adv: Œº=0.000 œÉ=1.000
Update  190 | return=  -37.0 (n=2823) | loss=6.617 | policy=-0.086 | value=13.406 | entropy=1.188 (coef=0.0462) | adv: Œº=-0.000 œÉ=1.000
Update  200 | return=  -36.5 (n=2908) | loss=5.908 | policy=-0.120 | value=12.055 | entropy=1.176 (coef=0.0460) | adv: Œº=-0.000 œÉ=1.000
[EVAL] Update  200 | avg_return over 10 eps = -1184.4
Update  210 | return=  -39.2 (n=2973) | loss=3.593 | policy=-0.052 | value=7.291 | entropy=1.153 (coef=0.0458) | adv: Œº=0.000 œÉ=1.000


‚ö†Ô∏è  Training interrupted by user (Ctrl+C)
Checkpoint may have been saved during training.
