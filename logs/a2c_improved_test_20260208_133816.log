================================================================================
üß™ TEST RAPIDE A2C AM√âLIOR√â - 100 updates (~5-10 minutes)
================================================================================
Rollout steps: 2048
Max updates: 100
Total steps: 204,800

üöÄ AM√âLIORATIONS ACTIV√âES:
  ‚úì Observation normalization (clip=¬±10.0)
  ‚úì Reward clipping (clip=¬±10.0)
  ‚úì Larger network (hidden=512)
  ‚úì AdamW optimizer (weight_decay=1e-05)
================================================================================


================================================================================
‚öôÔ∏è  CONFIGURATION
================================================================================
  env_id                    = LunarLander-v3
  seed                      = 42
  gamma                     = 0.99
  gae_lambda                = 0.95
  lr_policy                 = 0.0005
  lr_value                  = 0.001
  entropy_coef_start        = 0.05
  entropy_coef_final        = 0.005
  value_coef                = 0.5
  rollout_steps             = 2048
  max_updates               = 100
  eval_every                = 25
  eval_episodes             = 30
  hidden_size               = 512
  grad_clip                 = 0.5
  normalize_obs             = True
  reward_clip               = 10.0
  weight_decay              = 1e-05
  obs_clip                  = 10.0
  save_dir                  = checkpoints
  save_name                 = a2c_2048_10000.pt
  render_eval_human         = False
  record_video              = False
  video_dir                 = videos_record
  solved_mean_reward        = 200.0
  solved_window             = 100
================================================================================

[INFO] Device: cpu
[INFO] Training mode: A2C with GAE (rollout_steps=2048)
[INFO] PyTorch version: 2.8.0
[INFO] Gymnasium version: 1.2.3

[INFO] Observation normalization: ENABLED (clip=10.0)
[INFO] Reward clipping: ENABLED (clip=¬±10.0)

Update   10 | return=   -2.8 (n=138) | loss=3.789 | policy=-0.124 | value=7.827 | entropy=1.177 (coef=0.0450) | adv: Œº=-0.000 œÉ=1.000
Update   20 | return=   28.1 (n=179) | loss=1.657 | policy=-0.136 | value=3.586 | entropy=0.993 (coef=0.0400) | adv: Œº=-0.000 œÉ=1.000
