# üêõ Analyse des Bugs d'√âvaluation - A2C Lunar Lander

## üìä R√©sultats Observ√©s (Log 20260207_132338)

### **Progression G√©n√©rale**
```
Training time: 448.4 minutes (7.5 heures)
Total updates: 7500
Best eval: 172.0
Final mean: 125.1 ¬± 112.7
Status: ‚ùå NOT SOLVED (target: 200)
```

### **Evals Catastrophiques en D√©but**
```
Update   50 | train=-127.2 ‚Üí eval=-1527.9  ‚ùå (12x pire!)
Update  100 | train=-82.9  ‚Üí eval=-3077.8  ‚ùå (37x pire!)
Update  150 | train=-57.8  ‚Üí eval=-396.8   ‚ùå (7x pire!)
Update  200 | train=-30.2  ‚Üí eval=-278.7   ‚ùå (9x pire!)
Update  400 | train=+8.4   ‚Üí eval=-216.2   ‚ùå (26x pire!)
...
Update 7450 | train=134.2  ‚Üí eval=172.0    ‚úÖ (meilleur)
Update 7500 | train=125.1  ‚Üí eval=121.8    ‚úÖ (coh√©rent)
```

---

## üîç BUG #1: √âvaluations Extr√™mement N√©gatives

### **Probl√®me Identifi√©**

Les returns d'√©val en d√©but d'entra√Ænement sont 10-37x plus mauvais que le training. Cela sugg√®re que certains √©pisodes d'√©valuation accumulent des p√©nalit√©s √âNORMES.

### **Cause Probable: Time Limit**

LunarLander-v3 a un time limit de **1000 steps** par d√©faut. Si la policy est tr√®s mauvaise au d√©but:

1. Agent crash rapidement ‚Üí -100 reward (normal)
2. **OU** Agent reste en vol sans atterrir ‚Üí accumule des p√©nalit√©s chaque step

**Exemple de catastrophe:**
```python
# Policy tr√®s mauvaise qui fait juste hover
# Chaque step: reward ‚âà -0.3 (carburant) - 0.3 (distance) = -0.6
# Sur 1000 steps: -0.6 * 1000 = -600 reward ‚ùå

# Pire: si plusieurs √©pisodes comme √ßa
# 10 √©pisodes x -600 = -6000 / 10 = -600 moyenne ‚ùå
# Mais si 1 √©pisode √† -3000 (bug?) + 9 autres √† -100:
# (-3000 + 9*(-100)) / 10 = -3900 / 10 = -390 ‚ùå
```

### **V√©rification dans le Code**

La fonction `evaluate()` ne semble PAS avoir de protection contre les √©pisodes qui ne terminent jamais:

```python
# A2C.py ligne 546-565
def evaluate(cfg: Config, policy: PolicyNet, device: torch.device = torch.device('cpu')) -> float:
    env = make_eval_env(cfg)  # ‚Üê Cr√©e env avec render_mode
    returns = []
    for _ in range(cfg.eval_episodes):
        obs, _ = env.reset()  # ‚Üê PAS de seed fixe (OK maintenant)
        done = False
        ep_return = 0.0
        while not done:  # ‚Üê Peut boucler jusqu'√† time limit
            # ...
            obs, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            ep_return += float(reward)  # ‚Üê Accumule toutes les p√©nalit√©s
        returns.append(ep_return)
    return float(np.mean(returns))
```

**Le probl√®me**: Si la policy au d√©but est mauvaise et fait juste hover sans jamais atterrir, l'√©pisode va jusqu'au time limit (1000 steps) et accumule -600 √† -1000 de p√©nalit√©s.

### **Solutions Propos√©es**

#### **Solution 1: Augmenter eval_episodes (recommand√©)**
```python
cfg.eval_episodes = 30  # Au lieu de 10
```
- R√©duit la variance caus√©e par 1-2 √©pisodes catastrophiques
- Plus repr√©sentatif de la vraie performance

#### **Solution 2: Clipper les returns d'eval**
```python
# Dans evaluate()
returns.append(max(ep_return, -500))  # Clip les catastrophes
```
- Emp√™che un seul √©pisode catastrophique de dominer la moyenne

#### **Solution 3: Logger les stats d√©taill√©es**
```python
# Dans evaluate()
print(f"[EVAL DEBUG] Episode returns: {returns}")
print(f"[EVAL DEBUG] Min: {min(returns):.1f}, Max: {max(returns):.1f}, Std: {np.std(returns):.1f}")
```
- Permet d'identifier si le probl√®me vient de quelques outliers

#### **Solution 4: Early stopping des √©pisodes pathologiques**
```python
# Dans evaluate()
MAX_STEPS = 1000  # Limit explicite
step_count = 0
while not done and step_count < MAX_STEPS:
    # ...
    step_count += 1
if step_count >= MAX_STEPS:
    print(f"[EVAL WARNING] Episode truncated at {MAX_STEPS} steps")
```

---

## üîç BUG #2: Entropy Reste Haute

### **Observation**
```
Update    0: entropy=1.381 (max, policy uniforme)
Update 7500: entropy=0.740 (toujours haute!)
  entropy_coef=0.001 (minimal)
```

**Attendu**: Entropy devrait descendre vers 0.3-0.4 pour une policy d√©cisive.

### **Probl√®me**

L'entropy NE DESCEND PAS malgr√©:
- 7500 updates d'entra√Ænement
- entropy_coef annealed √† 0.001 (quasi nul)
- Training return plateau √† ~125

**Cela signifie:** La policy reste tr√®s stochastique (ind√©cise).

### **Causes Possibles**

1. **Annealing trop rapide**
   - entropy_coef passe de 0.05 √† 0.001 sur 7500 updates
   - Progress = update / 7500
   - √Ä update 3750: coef ‚âà 0.025 (d√©j√† moiti√©)
   - La policy n'a pas eu assez de temps pour explorer avant que le bonus entropy disparaisse

2. **Policy gradient trop faible vs entropy**
   - Si les advantages sont petits, le gradient de policy est faible
   - L'entropy domine et emp√™che la convergence

3. **Learning rate trop bas**
   - lr_policy = 3e-4 pourrait √™tre trop bas
   - La policy n'apprend pas assez vite

### **Solutions Propos√©es**

#### **Solution 1: Ralentir l'annealing (RECOMMAND√â)**
```python
# Config
entropy_coef_start: float = 0.05
entropy_coef_final: float = 0.005  # Au lieu de 0.001
max_updates: int = 10000  # Au lieu de 7500

# Ou changer la formule d'annealing
# Lin√©aire ‚Üí Exponentiel (reste haut plus longtemps)
entropy_coef = max(
    cfg.entropy_coef_final,
    cfg.entropy_coef_start * (0.995 ** update_idx)  # D√©croissance exp
)
```

#### **Solution 2: Augmenter lr_policy**
```python
lr_policy: float = 5e-4  # Au lieu de 3e-4
```

#### **Solution 3: Augmenter value_coef**
```python
value_coef: float = 1.0  # Au lieu de 0.5
```
- Force le critique √† converger plus vite
- Advantages plus stables ‚Üí policy gradient plus fort

---

## üîç BUG #3: Value Loss Reste √âlev√©e

### **Observation**
```
Update    0: value_loss=32.224 (tr√®s haute)
Update 7500: value_loss=7.322  (toujours haute)
```

**Attendu**: value_loss < 2.0 pour convergence

### **Probl√®me**

Le critique ne converge PAS. Cela signifie:
- Les predictions de valeur sont mauvaises
- Les advantages sont bruit√©s
- Le policy gradient est instable

### **Causes Possibles**

1. **Learning rate trop bas**
   - lr_value = 3e-4 pourrait √™tre trop bas
   - Le critique n'apprend pas assez vite

2. **Targets instables (GAE)**
   - Si les returns calcul√©s par GAE sont bruyants
   - Le critique ne peut pas apprendre

3. **Network trop petit**
   - hidden_size = 128 pourrait √™tre insuffisant
   - Le critique ne peut pas approximer la value function

### **Solutions Propos√©es**

#### **Solution 1: Augmenter lr_value**
```python
lr_value: float = 1e-3  # Au lieu de 3e-4
```

#### **Solution 2: Augmenter hidden_size**
```python
hidden_size: int = 256  # Au lieu de 128
```

#### **Solution 3: R√©duire GAE lambda**
```python
gae_lambda: float = 0.9  # Au lieu de 0.95
```
- Moins de variance dans les returns
- Mais plus de bias

---

## üîç BUG #4: Plateau √† ~120-140

### **Observation**
```
Update 6000-7500: returns oscillent entre 60-140
Best eval: 172.0 (update 7450)
Final: 121.8 (update 7500)
Jamais atteint 200
```

### **Probl√®me**

L'agent plateau et ne progresse plus apr√®s update ~5000.

### **Causes Cumulatives**

1. Entropy trop haute ‚Üí policy ind√©cise
2. Value loss haute ‚Üí gradients bruit√©s
3. Annealing trop rapide ‚Üí exploration arr√™t√©e trop t√¥t
4. Network trop petit ‚Üí capacit√© limit√©e

### **Solutions**

Appliquer TOUTES les corrections pr√©c√©dentes:
1. Ralentir entropy annealing
2. Augmenter lr_policy et lr_value
3. Augmenter hidden_size
4. Augmenter eval_episodes
5. Plus d'updates (10000 au lieu de 7500)

---

## üìã PLAN D'ACTION RECOMMAND√â

### **üî• Corrections Urgentes (Priorit√© 1)**

```python
@dataclass
class Config:
    # ... autres params ...

    # 1. Ralentir entropy annealing
    entropy_coef_start: float = 0.05
    entropy_coef_final: float = 0.005  # ‚Üê Au lieu de 0.001

    # 2. Plus d'updates
    max_updates: int = 10000  # ‚Üê Au lieu de 7500

    # 3. Plus d'√©pisodes d'eval (r√©duire variance)
    eval_episodes: int = 30  # ‚Üê Au lieu de 10

    # 4. Augmenter LR du critique
    lr_value: float = 1e-3  # ‚Üê Au lieu de 3e-4
```

### **‚ö° Corrections Importantes (Priorit√© 2)**

```python
@dataclass
class Config:
    # 5. Network plus large
    hidden_size: int = 256  # ‚Üê Au lieu de 128

    # 6. Augmenter LR de la policy
    lr_policy: float = 5e-4  # ‚Üê Au lieu de 3e-4
```

### **üîß Debug/Monitoring (Priorit√© 3)**

Ajouter dans `evaluate()`:
```python
def evaluate(cfg: Config, policy: PolicyNet, device: torch.device = torch.device('cpu')) -> float:
    env = make_eval_env(cfg)
    returns = []
    for ep_idx in range(cfg.eval_episodes):
        # ... code existant ...
        returns.append(ep_return)

        # DEBUG: Log outliers
        if ep_return < -500:
            print(f"[EVAL WARNING] Episode {ep_idx+1} extreme: {ep_return:.1f}")

    env.close()

    # DEBUG: Log stats
    mean_ret = float(np.mean(returns))
    std_ret = float(np.std(returns))
    min_ret = float(np.min(returns))
    max_ret = float(np.max(returns))

    if update_idx % 100 == 0:  # Toutes les 100 updates
        print(f"[EVAL STATS] mean={mean_ret:.1f} std={std_ret:.1f} min={min_ret:.1f} max={max_ret:.1f}")

    return mean_ret
```

---

## üìä R√©sultats Attendus Apr√®s Corrections

### **Avant (Actuel)**
```
Updates: 7500
Training time: 7.5 heures
Best eval: 172.0
Final: 121.8
Entropy (final): 0.74
Value loss (final): 7.3
Status: ‚ùå NOT SOLVED
```

### **Apr√®s (Attendu)**
```
Updates: ~5000-8000
Training time: ~5-6 heures
Best eval: > 220
Final: > 200
Entropy (final): 0.3-0.4
Value loss (final): < 2.0
Status: ‚úÖ SOLVED
```

---

## ‚úÖ Checklist de V√©rification

Apr√®s r√©entra√Ænement avec corrections:

- [ ] Eval episodes augment√© √† 30
- [ ] Pas d'evals < -500 dans les logs
- [ ] Entropy descend < 0.5 vers update 5000
- [ ] Value loss < 3.0 vers update 3000
- [ ] Train et eval concordent (√©cart < 30)
- [ ] Plateau d√©pass√© (returns > 150)
- [ ] Solved (eval > 200)

---

## üéØ Conclusion

Les probl√®mes principaux sont:
1. **Eval instable** due √† variance √©lev√©e (10 √©pisodes) et outliers catastrophiques
2. **Entropy annealing trop rapide** emp√™che convergence
3. **Value loss haute** indique critique qui n'apprend pas bien
4. **Network/LR sous-dimensionn√©s** pour la complexit√© du probl√®me

Avec les corrections propos√©es, vous devriez atteindre 200+ reward en ~5000-8000 updates. üöÄ
