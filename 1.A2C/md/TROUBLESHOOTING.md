# üîß Guide de D√©pannage - A2C Lunar Lander

## ‚ö†Ô∏è "Les r√©sultats sont pires qu'avant"

### **Diagnostic Rapide**

#### **Sc√©nario 1: Vous testez l'ancien checkpoint (256 hidden)**
**Sympt√¥mes:**
```bash
RuntimeError: size mismatch for net.0.weight
```
ou performances d√©grad√©es avec l'ancien checkpoint.

**Cause:**
- Code actuel utilise `hidden_size=512`
- Ancien checkpoint a `hidden_size=256`
- OU normalizer activ√© mais pas dans le checkpoint

**Solution:**
```bash
# Option A: Utiliser le baseline exact
python A2C_baseline.py

# Option B: Tester l'ancien checkpoint correctement
# Le load_policy() devrait g√©rer √ßa automatiquement maintenant
```

---

#### **Sc√©nario 2: Nouvel entra√Ænement pire que baseline**
**Sympt√¥mes:**
- Returns stagnent √† ~50-100 (vs 200 avant)
- Entropy reste > 1.0 apr√®s 1000 updates
- Value loss > 10 apr√®s 3000 updates

**Causes possibles:**

##### **A. Normalizer Instable en D√©but**
Le normalizer d√©marre avec mean=0, var=1 (donn√©es insuffisantes).

**Solution:**
```python
# D√©sactivez temporairement
cfg.normalize_obs = False
```

##### **B. Reward Clipping Trop Agressif**
Clip √† ¬±10 peut trop r√©duire le signal d'apprentissage.

**Solution:**
```python
# D√©sactivez ou augmentez
cfg.reward_clip = None  # Ou 20.0
```

##### **C. Network 512 Trop Large**
N√©cessite plus de donn√©es pour converger.

**Solution:**
```python
# R√©duisez √† 384 (compromis)
cfg.hidden_size = 384
```

##### **D. Weight Decay Trop Fort**
1e-5 peut trop r√©gulariser en d√©but.

**Solution:**
```python
# R√©duisez
cfg.weight_decay = 1e-6  # Ou 0.0
```

---

## üìä Comparaison des Configurations

| Config | Hidden | Normalize | Reward Clip | Weight Decay | Attendu |
|--------|--------|-----------|-------------|--------------|---------|
| **Baseline** | 256 | ‚ùå | ‚ùå | 0.0 | 200, 74.8% ‚úÖ PROUV√â |
| **Gradual** | 384 | ‚ùå | ‚ùå | 1e-6 | 210, 78-80% |
| **Full** | 512 | ‚úÖ | 10.0 | 1e-5 | 220, 90%+ |

---

## üß™ Plan de Test Syst√©matique

### **√âtape 1: Reproduire Baseline (OBLIGATOIRE)**
```bash
python A2C_baseline.py
```

**R√©sultat attendu:**
- Best eval: ~220
- Final: ~200
- Success: 74-76%

**Si √ßa ne marche PAS:**
‚Üí Probl√®me ailleurs (environment, seed, PyTorch version)
‚Üí STOP et debug

**Si √ßa marche:**
‚Üí Continuez √©tape 2

---

### **√âtape 2: Tester Hidden Size Augment√©**
```bash
python A2C_gradual.py  # hidden=384
```

**R√©sultat attendu:**
- Best eval: 220-230
- Final: 205-215
- Success: 78-82%

**Si meilleur que baseline:**
‚Üí Hidden size aide! Continuez √©tape 3

**Si pareil:**
‚Üí Hidden size ne change rien, essayez normalisation

**Si pire:**
‚Üí Revenez √† 256, probl√®me de convergence

---

### **√âtape 3: Ajouter Normalisation**

Modifiez `A2C_gradual.py`:
```python
cfg.hidden_size = 384  # ou 512 si √©tape 2 r√©ussie
cfg.normalize_obs = True  # ‚Üê NOUVEAU
cfg.reward_clip = None    # Pas encore
cfg.weight_decay = 1e-6   # Minimal
```

**R√©sultat attendu:**
- Best eval: 230-240
- Final: 215-225
- Success: 85-90%

**Si meilleur:**
‚Üí Normalisation aide! Continuez √©tape 4

**Si instable (NaN, diverge):**
‚Üí Probl√®me avec normalizer, d√©sactivez

---

### **√âtape 4: Ajouter Reward Clipping**

```python
cfg.normalize_obs = True
cfg.reward_clip = 15.0  # ‚Üê Moins agressif que 10.0
```

**R√©sultat attendu:**
- Best eval: 235-250
- Final: 220-230
- Success: 88-93%

---

### **√âtape 5: Full Config (Si Tout Marche)**

```python
cfg.hidden_size = 512
cfg.normalize_obs = True
cfg.reward_clip = 10.0
cfg.weight_decay = 1e-5
```

**R√©sultat attendu:**
- Best eval: 250+
- Final: 230+
- Success: 90-95%

---

## üö® Signaux d'Alerte

### **CRITIQUE - STOP Imm√©diatement**
```
Update 100 | return= NaN  ‚ùå DIVERGENCE
Update 500 | value=45.2   ‚ùå Value loss explose
[EVAL] avg_return = -2000 ‚ùå Eval catastrophique
```

**Action:**
1. Ctrl+C pour arr√™ter
2. Revenir au baseline
3. D√©sactiver TOUTES les am√©liorations
4. Chercher bug dans le code

---

### **WARNING - Surveiller**
```
Update 2000 | entropy=1.1  ‚ö†Ô∏è Entropy trop haute
Update 3000 | return=50    ‚ö†Ô∏è Stagnation
[EVAL STATS] std=150.2     ‚ö†Ô∏è Variance tr√®s haute
```

**Action:**
1. Laisser entra√Æner jusqu'√† update 5000
2. Si pas d'am√©lioration ‚Üí revert une am√©lioration
3. Essayer learning rate plus √©lev√©

---

## üîç Debugging Checklist

Si performances pires qu'attendu:

- [ ] **V√©rifier seed:** M√™me seed (42) utilis√©?
- [ ] **V√©rifier env:** LunarLander-v3 (pas v2)?
- [ ] **V√©rifier PyTorch:** Version compatible?
- [ ] **V√©rifier logs:**
  - `[INFO] Observation normalization: ENABLED` si normalize_obs=True
  - `[INFO] Reward clipping: ENABLED` si reward_clip d√©fini
- [ ] **Comparer configs:**
  ```python
  # Dans le log, section CONFIGURATION
  # Comparez avec baseline r√©ussi
  ```
- [ ] **V√©rifier gradients:**
  ```
  adv: Œº‚âà0.000 œÉ‚âà1.000  ‚úÖ OK
  adv: Œº=2.5 œÉ=15.2     ‚ùå PAS OK (advantages non normalis√©s)
  ```

---

## üí° Optimisations Alternatives

Si AUCUNE am√©lioration ne marche:

### **Option 1: Tuning Hyperparam√®tres**
```python
# Essayez des LR diff√©rents
cfg.lr_policy = 7e-4  # Au lieu de 5e-4
cfg.lr_value = 1.5e-3  # Au lieu de 1e-3

# Ou entropy annealing plus lent
cfg.entropy_coef_final = 0.01  # Au lieu de 0.005
```

### **Option 2: Plus d'Entra√Ænement**
```python
cfg.max_updates = 15000  # Au lieu de 10000
```

### **Option 3: Changer Algo**
- Passer √† **PPO** (plus stable)
- Ou **SAC** (state-of-the-art)

---

## üìà R√©sultats de R√©f√©rence

### **Baseline (PROUV√â)**
```
Config: 256 hidden, no improvements
Updates: 8484
Time: 62.4 min
Best eval: 220.2
Final: 200.1 ¬± 99.9
Success: 74.8%
Status: ‚úÖ SOLVED
```

### **Objectif avec Am√©liorations**
```
Config: 512 hidden, all improvements
Updates: 6000-7000
Time: 50-60 min
Best eval: 250+
Final: 220 ¬± 50
Success: 90-95%
Status: ‚úÖ SOLVED STABLE
```

**Si vous n'atteignez pas ces r√©sultats:**
1. Revenez au baseline
2. V√©rifiez que baseline fonctionne
3. Ajoutez am√©liorations UNE par UNE
4. Testez chaque changement s√©par√©ment

---

## üÜò Support

**Si tout √©choue:**

1. Partagez votre log file complet
2. Partagez votre configuration exacte
3. Indiquez:
   - R√©sultats attendus vs obtenus
   - √âtape o√π √ßa bloque
   - Messages d'erreur

**Commandes utiles pour debug:**
```bash
# Comparer deux logs
diff logs/a2c_baseline_*.log logs/a2c_gradual_*.log

# Voir derni√®res 50 lignes
tail -50 logs/a2c_*.log

# Chercher "SOLVED"
grep "SOLVED" logs/*.log

# Extraire best eval de tous les runs
grep "Best eval reward" logs/*.log
```

---

## ‚úÖ Rappel: Commencez Simple!

**N'essayez PAS d'appliquer toutes les am√©liorations d'un coup.**

**Workflow recommand√©:**
1. ‚úÖ Baseline (256, no improvements) ‚Üí **74.8% success**
2. ‚úÖ +Hidden size (384) ‚Üí **78-82% success**
3. ‚úÖ +Normalization ‚Üí **85-90% success**
4. ‚úÖ +Reward clip ‚Üí **90-95% success**

Chaque √©tape doit AM√âLIORER les r√©sultats, sinon STOP et debug!

---

**Bonne chance! üöÄ**
